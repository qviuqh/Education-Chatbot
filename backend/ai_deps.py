"""
AI Model Dependencies
---------------------
Cung c·∫•p c√°c h√†m singleton cho c√°c model AI (Embedder, Reranker, Ollama client).
C√°c instance n√†y ƒë∆∞·ª£c kh·ªüi t·∫°o m·ªôt l·∫ßn v√† d√πng chung cho to√†n b·ªô ·ª©ng d·ª•ng.
"""
import ollama
from functools import lru_cache
from ollama import Client

from .config import settings
from .rag_pipeline.embedder import Embedder
from .rag_pipeline.reranker import Reranker


@lru_cache(maxsize=1)
def get_embedder() -> Embedder:
    """
    Tr·∫£ v·ªÅ Embedder d√πng chung.
    L∆ØU √ù: Ch·∫°y tr√™n CPU ƒë·ªÉ d√†nh VRAM cho Ollama (Generator)
    """
    embedder = Embedder(model_name=settings.EMBEDDING_MODEL, device="cpu")
    print("‚úÖ ƒê√£ t·∫£i xong model Embedding.", flush=True)
    return embedder


@lru_cache(maxsize=1)
def get_reranker() -> Reranker:
    """
    Tr·∫£ v·ªÅ Reranker d√πng chung (t·∫°o khi c·∫ßn).
    L∆ØU √ù: Ch·∫°y tr√™n CPU ƒë·ªÉ d√†nh VRAM cho Ollama (Generator)
    """
    # Reranker class c·ªßa b·∫°n ƒë√£ c√≥ logic nh·∫≠n tham s·ªë device (xem file reranker.py c≈©)
    reranker = Reranker(model_name=settings.RERANKER_MODEL, device="cpu")
    print("‚úÖ ƒê√£ t·∫£i xong model Reranker.", flush=True)
    return reranker


@lru_cache(maxsize=1)
def get_ollama_client() -> Client:
    """Tr·∫£ v·ªÅ Ollama client d√πng chung."""
    return Client(host=settings.OLLAMA_BASE_URL)

def check_model_downloaded(model_name):
    try:
        models = ollama.list()
        for model_info in models['models']:
            if model_info['name'].startswith(model_name):
                print(f"Model '{model_name}' is downloaded.")
                return True
        print(f"Model '{model_name}' is not downloaded.")
        return False
    except Exception as e:
        print(f"Error checking models: {e}")
        return False

def warmup_ai_models() -> None:
    """Kh·ªüi t·∫°o s·∫µn c√°c model AI khi server start."""
    print("WARMUP: Initializing models on CPU to save VRAM for Ollama...")
    get_embedder()
    if settings.USE_RERANKER:
        get_reranker()
    client = get_ollama_client()
    model_name = settings.LLM_MODEL
    
    print(f"‚è≥ ƒêang ki·ªÉm tra model Ollama: {model_name}...", flush=True)
    
    if not check_model_downloaded(model_name):
        try:
            print(f"   ƒêang g·ª≠i l·ªánh pull '{model_name}' t·ªõi Ollama (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...", flush=True)
            
            # Th·ª±c hi·ªán pull
            progress = client.pull(model_name, stream=True)
            
            # In ti·∫øn tr√¨nh ƒë∆°n gi·∫£n ƒë·ªÉ log
            for chunk in progress:
                if chunk.get('status') == 'success':
                    print(f"   üöÄ ƒê√£ t·∫£i xong layer: {chunk.get('digest', '')[:12]}...", flush=True)
            
            print(f"‚úÖ Model Ollama '{model_name}' ƒë√£ s·∫µn s√†ng!", flush=True)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi pull model Ollama: {e}", flush=True)
            print("   Vui l√≤ng ƒë·∫£m b·∫£o container 'ollama' ƒëang ch·∫°y v√† c√≥ k·∫øt n·ªëi m·∫°ng.", flush=True)