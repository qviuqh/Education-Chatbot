import json
import os
import sys
import pandas as pd
from pathlib import Path
from typing import List, Dict
from datetime import datetime

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n import ---
current_dir = Path(__file__).resolve().parent
sys.path.append(str(current_dir))

# --- Import Libraries ---
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

# Import Wrappers cho Local Models
try:
    from ragas.llms import llm_factory
    from ragas.embeddings import HuggingfaceEmbeddings
    USE_NEW_RAGAS = True
except ImportError:
    USE_NEW_RAGAS = False

# Import RAG Components t·ª´ Backend
from backend.rag_pipeline.rag import RAGRetriever, answer_question_with_store
from backend.ai_deps import get_embedder
from backend.config import settings


class RagasEvaluator:
    """
    Class ƒë·ªÉ ƒë√°nh gi√° RAG pipeline s·ª≠ d·ª•ng Local LLM (Ollama)
    """
    
    def __init__(
        self, 
        index_path: str,
        meta_path: str,
        test_data_path: str = "test_data.json",
        llm_model: str = None,
        embedding_model: str = None
    ):
        """
        Args:
            index_path: ƒê∆∞·ªùng d·∫´n file .index
            meta_path: ƒê∆∞·ªùng d·∫´n file .json
            test_data_path: ƒê∆∞·ªùng d·∫´n file test data
            llm_model: T√™n model LLM (m·∫∑c ƒë·ªãnh t·ª´ settings)
            embedding_model: T√™n model embedding (m·∫∑c ƒë·ªãnh t·ª´ settings)
        """
        # 1. Kh·ªüi t·∫°o Pipeline
        print("üöÄ ƒêang kh·ªüi t·∫°o RAG components th·ª±c t·∫ø...")
        self.system_embedder = get_embedder()
        
        if not os.path.exists(index_path) or not os.path.exists(meta_path):
            raise FileNotFoundError(
                f"Kh√¥ng t√¨m th·∫•y file index ho·∫∑c meta t·∫°i:\n"
                f"  Index: {index_path}\n"
                f"  Meta:  {meta_path}"
            )
            
        self.retriever = RAGRetriever(index_path, meta_path, self.system_embedder)
        
        # 2. Load Test Data
        print(f"üìÇ ƒêang t·∫£i d·ªØ li·ªáu test t·ª´ {test_data_path}...")
        try:
            with open(test_data_path, 'r', encoding='utf-8') as f:
                self.test_data = json.load(f)
            print(f"‚úÖ ƒê√£ t·∫£i {len(self.test_data['questions'])} c√¢u h·ªèi test")
        except FileNotFoundError:
            raise FileNotFoundError(
                f"Kh√¥ng t√¨m th·∫•y file {test_data_path}\n"
                f"Vui l√≤ng t·∫°o file test data tr∆∞·ªõc khi ch·∫°y."
            )

        # 3. C·∫•u h√¨nh Judge Models
        self.llm_model = llm_model or settings.LLM_MODEL
        self.embedding_model = embedding_model or settings.EMBEDDING_MODEL
        
        print(f"‚öñÔ∏è  C·∫•u h√¨nh Ragas Judge:")
        print(f"   LLM Model: {self.llm_model}")
        print(f"   Embedding: {self.embedding_model}")
        print(f"   Ragas API: {'New (llm_factory)' if USE_NEW_RAGAS else 'Legacy (Wrapper)'}")
        
        # Setup Judge LLM v√† Embeddings
        self._setup_judge_models()

    def _setup_judge_models(self):
        """Setup LLM v√† Embedding models cho Ragas"""
        global USE_NEW_RAGAS
        
        if USE_NEW_RAGAS:
            print("‚ö†Ô∏è  Ragas m·ªõi ch∆∞a h·ªó tr·ª£ t·ªët Ollama, ƒëang fallback v·ªÅ wrapper...")
            USE_NEW_RAGAS = False
        
        # Import wrapper
        from ragas.llms import LangchainLLMWrapper
        from ragas.embeddings import LangchainEmbeddingsWrapper
        from langchain_ollama import ChatOllama
        from langchain_huggingface import HuggingFaceEmbeddings
        
        # Config
        self.judge_llm = LangchainLLMWrapper(
            ChatOllama(
                model=self.llm_model,
                base_url=settings.OLLAMA_BASE_URL,
                temperature=0,
                timeout=7200,  # 2h
                num_ctx=4096,
            )
        )
        
        self.judge_embeddings = LangchainEmbeddingsWrapper(
            HuggingFaceEmbeddings(
                model_name=self.embedding_model,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        )

    def generate_rag_responses(self, use_reranker: bool = False) -> List[Dict]:
        """
        Ch·∫°y RAG pipeline ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
        """
        print("\n" + "="*60)
        print("üîÑ ƒêang ch·∫°y RAG Pipeline ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi...")
        print(f"   Use Reranker: {use_reranker}")
        print("="*60 + "\n")
        
        results = []
        
        for i, question in enumerate(self.test_data['questions'], 1):
            print(f"[{i}/{len(self.test_data['questions'])}] H·ªèi: {question}")
            
            try:
                # 1. Retrieve contexts
                retrieved_contexts_raw = self.retriever.retrieve(
                    question=question,
                    k_semantic=settings.TOP_K_RETRIEVE,
                    k_keyword=settings.TOP_K_RETRIEVE,
                    use_validation=False
                )
                
                # X·ª≠ l√Ω contexts
                if not retrieved_contexts_raw:
                    contexts_list = ["Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."]
                    print("   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y context")
                else:
                    contexts_list = []
                    for ctx in retrieved_contexts_raw:
                        if '\n' in ctx:
                            text = ctx.split('\n', 1)[1]
                        else:
                            text = ctx
                        contexts_list.append(text.strip())
                    
                    print(f"   ‚úì Contexts: {len(contexts_list)}")

                # 2. Generate Answer
                answer = answer_question_with_store(
                    question=question,
                    retriever=self.retriever,
                    streaming=False,
                    use_reranker=use_reranker,
                    detect_language=True
                )
                
                print(f"   ‚úì Answer: {answer[:80]}...")
                
                results.append({
                    "question": question,
                    "contexts": contexts_list,
                    "answer": answer,
                    "ground_truth": self.test_data["ground_truths"][i-1][0]
                })
                
            except Exception as e:
                print(f"   ‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
                results.append({
                    "question": question,
                    "contexts": ["Error during retrieval"],
                    "answer": f"L·ªói: {str(e)}",
                    "ground_truth": self.test_data["ground_truths"][i-1][0]
                })
        
        return results

    def save_detailed_results(self, results, rag_results, output_dir: str = "evaluation_results"):
        """
        L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° chi ti·∫øt v√†o c√°c file CSV
        """
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # 1. FILE SCORES: ƒêi·ªÉm t·ª´ng c√¢u h·ªèi
            if hasattr(results, 'to_pandas'):
                df_scores = results.to_pandas()
            else:
                df_scores = pd.DataFrame([results])
            
            # L·∫•y c√°c c·ªôt metric
            metric_cols = [col for col in df_scores.columns 
                          if col not in ['question', 'answer', 'contexts', 'ground_truth']]
            
            # T·∫°o DataFrame ƒëi·ªÉm
            scores_data = []
            for idx, row in df_scores.iterrows():
                score_row = {
                    'question_id': idx + 1,
                    'question': row.get('question', rag_results[idx]['question']),
                }
                
                for metric in metric_cols:
                    if metric in row:
                        score_row[metric] = row[metric]
                
                scores_data.append(score_row)
            
            df_final_scores = pd.DataFrame(scores_data)
            scores_file = f"{output_dir}/scores_{timestamp}.csv"
            df_final_scores.to_csv(scores_file, index=False, encoding='utf-8-sig')
            print(f"\nüìä ƒê√£ l∆∞u ƒëi·ªÉm chi ti·∫øt v√†o: {scores_file}")
            
            # 2. FILE SUMMARY: Th·ªëng k√™ t·ªïng h·ª£p
            summary_data = []
            for metric in metric_cols:
                if metric in df_scores.columns:
                    values = df_scores[metric].dropna()
                    if len(values) > 0:
                        summary_data.append({
                            'metric': metric,
                            'mean': values.mean(),
                            'std': values.std(),
                            'min': values.min(),
                            'max': values.max(),
                            'count': len(values),
                            'nan_count': df_scores[metric].isna().sum()
                        })
            
            df_summary = pd.DataFrame(summary_data)
            summary_file = f"{output_dir}/summary_{timestamp}.csv"
            df_summary.to_csv(summary_file, index=False, encoding='utf-8-sig')
            print(f"üìà ƒê√£ l∆∞u th·ªëng k√™ t·ªïng h·ª£p v√†o: {summary_file}")
            
            # 3. FILE DETAILS: C√¢u h·ªèi, c√¢u tr·∫£ l·ªùi, contexts ƒë·∫ßy ƒë·ªß
            details_data = []
            for idx, result in enumerate(rag_results):
                detail_row = {
                    'question_id': idx + 1,
                    'question': result['question'],
                    'answer': result['answer'],
                    'ground_truth': result['ground_truth'],
                    'num_contexts': len(result['contexts']),
                    'contexts': ' ||| '.join(result['contexts'])
                }
                
                for metric in metric_cols:
                    if metric in df_scores.columns:
                        detail_row[f'score_{metric}'] = df_scores.iloc[idx][metric]
                
                details_data.append(detail_row)
            
            df_details = pd.DataFrame(details_data)
            details_file = f"{output_dir}/details_{timestamp}.csv"
            df_details.to_csv(details_file, index=False, encoding='utf-8-sig')
            print(f"üìù ƒê√£ l∆∞u chi ti·∫øt ƒë·∫ßy ƒë·ªß v√†o: {details_file}")
            
            # 4. FILE CONFIG: C·∫•u h√¨nh evaluation
            config_data = {
                'timestamp': [timestamp],
                'llm_model': [self.llm_model],
                'embedding_model': [self.embedding_model],
                'num_questions': [len(rag_results)],
                'metrics_used': [', '.join(metric_cols)],
                'index_path': [getattr(self.retriever, 'index_path', 'N/A')],
            }
            df_config = pd.DataFrame(config_data)
            config_file = f"{output_dir}/config_{timestamp}.csv"
            df_config.to_csv(config_file, index=False, encoding='utf-8-sig')
            print(f"‚öôÔ∏è  ƒê√£ l∆∞u c·∫•u h√¨nh v√†o: {config_file}")
            
            # 5. In t√≥m t·∫Øt ra terminal
            print("\n" + "="*70)
            print("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ ƒê√ÅNH GI√Å")
            print("="*70)
            print(f"\nüïí Th·ªùi gian: {timestamp}")
            print(f"üìÅ Th∆∞ m·ª•c k·∫øt qu·∫£: {output_dir}/")
            print(f"‚ùì S·ªë c√¢u h·ªèi: {len(rag_results)}")
            print(f"üìè Metrics: {', '.join(metric_cols)}")
            
            print("\nüìà ƒêI·ªÇM TRUNG B√åNH:")
            for _, row in df_summary.iterrows():
                metric = row['metric']
                mean = row['mean']
                
                if pd.isna(mean):
                    verdict = "‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu (NaN)"
                elif mean >= 0.8:
                    verdict = "‚úÖ EXCELLENT"
                elif mean >= 0.6:
                    verdict = "‚ö†Ô∏è  GOOD"
                elif mean >= 0.4:
                    verdict = "‚ö†Ô∏è  FAIR"
                else:
                    verdict = "‚ùå POOR"
                
                print(f"\n   {metric}:")
                print(f"      Mean:  {mean:.4f} ({verdict})")
                print(f"      Std:   {row['std']:.4f}")
                print(f"      Range: [{row['min']:.4f}, {row['max']:.4f}]")
                if row['nan_count'] > 0:
                    print(f"      ‚ö†Ô∏è  NaN: {row['nan_count']}/{row['count'] + row['nan_count']} c√¢u")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£: {e}")
            import traceback
            traceback.print_exc()
            return False

    def run_evaluation(
        self, 
        use_all_metrics: bool = False,
        use_reranker: bool = False,
        batch_size: int = 2
    ):
        """
        Th·ª±c hi·ªán ƒë√°nh gi√°
        """
        # 1. Thu th·∫≠p d·ªØ li·ªáu
        rag_results = self.generate_rag_responses(use_reranker=use_reranker)
        
        # 2. Chuy·ªÉn sang Dataset
        data_dict = {
            "question": [r["question"] for r in rag_results],
            "answer": [r["answer"] for r in rag_results],
            "contexts": [r["contexts"] for r in rag_results],
            "ground_truth": [r["ground_truth"] for r in rag_results]
        }
        dataset = Dataset.from_dict(data_dict)
        
        # 3. Ch·ªçn metrics
        if use_all_metrics:
            print("\n‚ö†Ô∏è  S·ª≠ d·ª•ng t·∫•t c·∫£ metrics - M·∫§T R·∫§T NHI·ªÄU TH·ªúI GIAN v·ªõi local LLM!")
            metrics = [
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
            ]
        else:
            print("\n‚úÖ Ch·ªâ d√πng answer_relevancy (nhanh nh·∫•t, kh√¥ng c·∫ßn LLM nhi·ªÅu)")
            metrics = [answer_relevancy]
        
        print("\n" + "="*60)
        print("üß™ ƒêang ch·∫•m ƒëi·ªÉm b·∫±ng Ragas...")
        print(f"   Metrics: {[m.name for m in metrics]}")
        print(f"   Dataset size: {len(dataset)}")
        print("="*60 + "\n")
        
        try:
            # TƒÉng timeout
            os.environ['RAGAS_TIMEOUT'] = '7200'
            
            results = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=self.judge_llm,
                embeddings=self.judge_embeddings,
                batch_size=batch_size,
            )
            
            # L∆∞u k·∫øt qu·∫£ chi ti·∫øt
            self.save_detailed_results(results, rag_results)
            
            return results
            
        except Exception as e:
            print(f"\n‚ùå L·ªói khi ƒë√°nh gi√°: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """Main function ƒë·ªÉ ch·∫°y evaluation"""
    print("\n" + "="*70)
    print("üéØ RAG PIPELINE EVALUATION WITH RAGAS (LOCAL LLM)")
    print("="*70 + "\n")
    
    # C·∫•u h√¨nh
    INDEX_FILE = "backend/data/vectordb/index.faiss" 
    META_FILE = "backend/data/vectordb/chunks.json"
    TEST_DATA_FILE = "test_data.json"
    OUTPUT_DIR = "evaluation_results"
    
    # Ki·ªÉm tra file index
    if not os.path.exists(INDEX_FILE):
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y {INDEX_FILE}")
        print("Th·ª≠ d√πng ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh...")
        INDEX_FILE = "data/vectordb/index.faiss" 
        META_FILE = "data/vectordb/chunks.json"
        
        if not os.path.exists(INDEX_FILE):
            print("‚ùå Kh√¥ng t√¨m th·∫•y file vector store n√†o!")
            return
    
    print(f"‚úÖ Target Vector Store: {INDEX_FILE}")
    
    # Ki·ªÉm tra test data
    if not os.path.exists(TEST_DATA_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y {TEST_DATA_FILE}")
        return

    try:
        evaluator = RagasEvaluator(
            index_path=INDEX_FILE,
            meta_path=META_FILE,
            test_data_path=TEST_DATA_FILE
        )
        
        print("\nüí° L∆ØU √ù:")
        print(f"   - K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c '{OUTPUT_DIR}/'")
        print("   - 4 file CSV: scores, summary, details, config")
        print("   - Ch·ªâ d√πng answer_relevancy ƒë·ªÉ tr√°nh timeout\n")
        
        # Ch·∫°y evaluation
        results = evaluator.run_evaluation(
            use_all_metrics=True,  # True ƒë·ªÉ test ƒë·∫ßy ƒë·ªß (r·∫•t ch·∫≠m)
            use_reranker=False,
            batch_size=1
        )
        
        if results:
            print("\n" + "="*70)
            print("‚úÖ HO√ÄN T·∫§T EVALUATION!")
            print(f"üìÇ Ki·ªÉm tra th∆∞ m·ª•c '{OUTPUT_DIR}/' ƒë·ªÉ xem k·∫øt qu·∫£ chi ti·∫øt")
            print("="*70)
        else:
            print("\n‚ùå Evaluation th·∫•t b·∫°i")

    except FileNotFoundError as e:
        print(f"\n‚ùå L·ªói file: {e}")
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()